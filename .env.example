# Backend: 'ollama' o 'groq'
BACKEND=ollama

# Temperatura de respuestas (0.0 = determinista, 1.0 = creativo)
TEMPERATURE=0.2

# Configuracion Ollama (local)
OLLAMA_BASE_URL=http://localhost:11434/v1/chat/completions
OLLAMA_MODEL=llama3

# Configuracion Groq (cloud)
GROQ_BASE_URL=https://api.groq.com/openai/v1/chat/completions
GROQ_MODEL=llama-3.1-8b-instant
GROQ_API_KEY=tu_api_key_aqui
